diff --git a/analyze.py b/analyze.py
index 0325d93..8e138e4 100644
--- a/analyze.py
+++ b/analyze.py
@@ -11,7 +11,9 @@ def extract_test_bleu(id):
     pat_bleu3 = re.compile('- BLEU-3 : ([^ \n]+)')
     pat_bleu4 = re.compile('- BLEU-4 : ([^ \n]+)')
     pat_epoch = re.compile('Starting epoch ([^ \n]+) \.\.\.')
-    path_train_log = '../poem-prose_dump/' + id + '/train.log'
+    pat_ppl = re.compile('- ppl_sw_pm_test -> ([^ \n]+)')
+
+    path_train_log = id + '/train.log'
     bleu_dic = {}
     with open(path_train_log) as f_in:
         con = f_in.read()
@@ -28,9 +30,10 @@ def extract_test_bleu(id):
             bleu2 = pat_bleu2.search(sub_con).group(1)
             bleu3 = pat_bleu3.search(sub_con).group(1)
             bleu4 = pat_bleu4.search(sub_con).group(1)
+            ppl = pat_ppl.search(sub_con).group(1)
             # print(' ' + id + '-' + epoch + ' & ' + bleu1 + ' & ' + bleu2 + ' & ' + bleu3  + ' & ' +   bleu4 + ' & ' +  bleu  + '\\\\')
             # print(' \\hline')
-            bleu_dic[int(epoch)] = [float(bleu1), float(bleu2), float(bleu3), float(bleu4), float(bleu)]
+            bleu_dic[int(epoch)] = [float(bleu1), float(bleu2), float(bleu3), float(bleu4), float(bleu), float(ppl)]
         except:
             break
     return bleu_dic
@@ -42,17 +45,22 @@ def show_bleu_score(model_name, epoches):
     bleu_lis2 = []
     bleu_lis3 = []
     bleu_lis4 = []
+    ppl = []
     for i in epoches:
         bleu_lis1.append(bleu_dic[i][0])
         bleu_lis2.append(bleu_dic[i][1])
         bleu_lis3.append(bleu_dic[i][2])
         bleu_lis4.append(bleu_dic[i][3])
-        bleu_lis.append(bleu_dic[i][-1])
+        bleu_lis.append(bleu_dic[i][4])
+        ppl.append(bleu_dic[i][5])
     print(model_name + ' 1-gram bleu\t' + str(np.mean(bleu_lis1)))
     print(model_name + ' 2-gram bleu\t' + str(np.mean(bleu_lis2)))
     print(model_name + ' 3-gram bleu\t' + str(np.mean(bleu_lis3)))
     print(model_name + ' 4-gram bleu\t' + str(np.mean(bleu_lis4)))
     print(model_name + '\t\t bleu' + str(np.mean(bleu_lis)))
+    print(model_name + '\t\t ppl' + str(np.mean(ppl)))
+    print(model_name + '\t\t bleus' + str(bleu_lis))
+    print(model_name + '\t\t ppls' + str(ppl))
     print('')
 
 def get_inter_ratio(path):
@@ -79,7 +87,7 @@ def get_repetition_ratio(path):
     with open(path) as f_in_:
         lines = f_in_.readlines()
     repetition_ratio_lis = []
-    for i in range(19200):
+    for i in range(960):
         line = lines[i].strip()
         repetition_ratio = 1.0 - len(set(line)) / float(len(line))
         repetition_ratio_lis.append(repetition_ratio)
@@ -88,7 +96,7 @@ def get_repetition_ratio(path):
 def show_repe_ratio(model_name, epoches):
     repe_ratio_lis = []
     for j in epoches:
-        path = '../poem-prose_dump/' + model_name + '/hyp' + str(j) + '.pm-sw.valid.txt'
+        path = model_name + '/hyp' + str(j) + '.pm-sw.valid.txt'
         repe_ratio_lis.append(get_repetition_ratio(path))
     print('repetition_ratio for ' + model_name + ':\t' + str(np.mean(repe_ratio_lis)))
 
@@ -97,30 +105,67 @@ def main():
     # rl10 0-41 + anti copy loss
     # rl18 9-37 + anti repetition loss
     # rl22 9-54 + anti copy loss & anti repetition loss
-    epoches = np.array(range(30, 36))
-    model1 = 'caibase'
-    model2 = 'cairl10'
-    model3 = 'cairl18'
-    model4 = 'cairl22'
-    show_bleu_score(model1, epoches)
-    show_bleu_score(model2, epoches)
-    show_bleu_score(model3, epoches-8)
-    show_bleu_score(model4, epoches-8)
-
-    show_repe_ratio(model1, epoches)
-    show_repe_ratio(model2, epoches)
-    show_repe_ratio(model3, epoches - 8)
-    show_repe_ratio(model4, epoches - 8)
-    print('repetition_ratio for real sanwen:\t' + str(get_repetition_ratio('./data/data_acc/sanwen.vl.txt')))
-
-    show_inter_ratio(model1, epoches)
-    show_inter_ratio(model2, epoches)
-    show_inter_ratio(model3, epoches-8)
-    show_inter_ratio(model4, epoches-8)
-
-
+    epoches = np.array(range(30, 35))
+    # epoches = np.array(range(25, 30))
 
+    model1 = '/mnt/nfs/work1/hongyu/pengshancai/exp/caibase'
+    model2 = '/mnt/nfs/work1/hongyu/pengshancai/exp/4902580'
+    model3 = './dumped/test/4948046'
+    # model3 = 'testlstm/4976233'
+    model4 = '/mnt/nfs/work1/hongyu/pengshancai/exp/4949781'
+    # model4 = 'testlstm/4976257'
 
+    show_bleu_score(model1, epoches)
+    show_bleu_score(model2, epoches-8)
+    show_bleu_score(model3, epoches)
+    show_bleu_score(model4, epoches)
+    show_repe_ratio(model1, epoches)
+    show_repe_ratio(model2, epoches-8)
+    show_repe_ratio(model3, epoches)
+    show_repe_ratio(model4, epoches)
+
+    # model1 = 'test/4977107'
+    # model2 = 'test/4982285'
+    # model3 = 'test/4982286'
+    # model4 = 'test/4948046'
+    # show_bleu_score(model1, epoches)
+    # show_bleu_score(model2, epoches-8)
+    # show_bleu_score(model3, epoches-4)
+    # show_bleu_score(model4, epoches)
+
+
+    # model1 = './dumped/test/4948046'
+    # model2 = './dumped/test/4977107'
+    # model3 = './dumped/test/4982285'
+    # model4 = './dumped/test/4982286'
+    # show_bleu_score(model1, epoches)
+    # show_bleu_score(model2, epoches)
+    # show_bleu_score(model3, epoches)
+    # show_bleu_score(model4, epoches)
+
+    # model1 = './dumped/test/caibase'
+    # model2 = './dumped/testlstm/4976233'
+    # model3 = './dumped/testlstm/4976257'
+    # show_bleu_score(model1, epoches)
+    # show_bleu_score(model2, epoches)
+    # show_bleu_score(model3, epoches)
+
+    
+
+
+    # show_repe_ratio(model1, epoches)
+    # show_repe_ratio(model2, epoches)
+    # show_repe_ratio(model3, epoches - 8)
+    # show_repe_ratio(model4, epoches - 8)
+    # print('repetition_ratio for real sanwen:\t' + str(get_repetition_ratio('./data/data_acc/sanwen.vl.txt')))
+
+    # show_inter_ratio(model1, epoches)
+    # show_inter_ratio(model2, epoches)
+    # show_inter_ratio(model3, epoches-8)
+    # show_inter_ratio(model4, epoches-8)
+
+
+main()
 
 
 
diff --git a/main.py b/main.py
index 8efd068..b6658e6 100644
--- a/main.py
+++ b/main.py
@@ -312,10 +312,14 @@ def main(params):
     if params.lm_before > 0:
         logger.info("Pretraining language model for %i iterations ..." % params.lm_before)
         trainer.n_sentences = 0
-        for _ in range(params.lm_before):
-            for lang in params.langs:
-                trainer.lm_step(lang)
+        for idx in range(params.lm_before):
+            for lang in params.mono_directions:
+                trainer.enc_dec_step(lang, lang, params.lambda_xe_mono)
+            # for lang in params.langs:
+            #     trainer.lm_step(lang)
             trainer.iter()
+            if (idx+1) % 10000 == 0:
+                trainer.save_model('lmpre_%d'%idx)
 
     # define epoch size
     if params.epoch_size == -1:
diff --git a/preprocess.py b/preprocess.py
index 90ad175..7cc02a2 100644
--- a/preprocess.py
+++ b/preprocess.py
@@ -142,7 +142,10 @@ def get_data(input_sents, tokenizer, issanwen, dopmpad):
             if dopmpad:
                 ind_len = len(indexed)
                 indexed = np.array(indexed)
-                sliced = list(range(2,ind_len+1,2))+list(range(2,ind_len+1,2))
+                # sliced = list(range(2,ind_len+1,2))+list(range(2,ind_len+1,2))
+                # sliced = list(range(8,ind_len+1,8))*8 #TODOTODO
+                # sliced = [3,8,11,16,19,24,27,32]*3+[5,13,21,29]*2
+                sliced = [5,8,13,16,21,24,29,32]*3+[2,10,18,26]*2
                 # logger.info(ind)
                 # logger.info(indexed.shape)
                 indexed = np.insert(indexed, sliced, [PADDING_IDX]*ind_len)
@@ -211,6 +214,17 @@ def get_data(input_sents, tokenizer, issanwen, dopmpad):
 # python preprocess.py data/vocab.txt data/poem7_out abc juejue pmpad 7
 # python preprocess.py data/vocab.txt data/poem_jueju7.txt  abc abc pmpad 7
 
+# mv data/poem7_out.tr.pth data/data_pad_4/jueju7_out.tr.pth
+# mv data/poem7_out.tr.txt data/data_pad_4/jueju7_out.tr.txt
+# mv data/poem7_out.tr.summary.txt data/data_pad_4/jueju7_out.tr.summary.txt
+# mv data/poem7_out.vl.pth data/data_pad_4/jueju7_out.vl.pth
+# mv data/poem7_out.vl.txt data/data_pad_4/jueju7_out.vl.txt
+# mv data/poem7_out.vl.summary.txt data/data_pad_4/jueju7_out.vl.summary.txt
+
+# mv data/poem_jueju7.pth data/data_pad_4/poem_jueju7_para.pm.pth
+# mv data/poem_jueju7.txt data/data_pad_4/poem_jueju7_para.pm.txt
+# mv data/poem_jueju7.txt.tr.summary.txt data/data_pad_4/poem_jueju7_para.pm.summary.txt.pm.txt
+
 if __name__ == '__main__':
 
     logger = create_logger(None)
@@ -333,7 +347,7 @@ if __name__ == '__main__':
         logger.info("Processing training data...")
         data, sent, sent_abs = get_data(train_sents, tokenizer, issanwen, dopmpad)
         # saveing data
-        logger.info("Saving the sent to %s ..." % txt_path+ '.tr.txt')
+        logger.info("Saving the sent to %s ..." % (txt_path+'.tr.txt'))
         with io.open(txt_path+ '.tr.txt', "w", encoding='utf8') as f:
             for line in sent:
                 f.write(line+'\n') 
@@ -372,7 +386,7 @@ if __name__ == '__main__':
         logger.info("Processing valid data...")
         data, sent, sent_abs = get_data(valid_sents, tokenizer, issanwen, dopmpad)
         # saveing data
-        logger.info("Saving the sent to %s ..." % txt_path+ '.vl.txt')
+        logger.info("Saving the sent to %s ..." % (txt_path+ '.vl.txt'))
         with io.open(txt_path+ '.vl.txt', "w", encoding='utf8') as f:
             for line in sent:
                 f.write(line+'\n') 
diff --git a/src/evaluator.py b/src/evaluator.py
index d08c323..d3924ef 100644
--- a/src/evaluator.py
+++ b/src/evaluator.py
@@ -58,8 +58,7 @@ class EvaluatorMT(object):
             end_sent_2_idx = ((7+1)*1+7)*2-1 #29
             end_sent_4_idx = ((7+1)*3+7)*2-1 #61
             # batch_ids[end_sent_2_idx,:]=self.params.blank_index
-            batch_ids[end_sent_4_idx,:]=self.params.blank_index
-            batch_ids[end_sent_4_idx-3,:]=self.params.blank_index
+            batch_ids[end_sent_4_idx-1:end_sent_4_idx+1,:]=self.params.blank_index
             # logger.info(batch_ids[28:31,:])
             # logger.info(batch_ids[60:63,:])
         else:
@@ -329,9 +328,10 @@ class EvaluatorMT(object):
         txt_tone_enh = []
 
         # for perplexity
-        loss_weight = self.decoder.loss_fn[lang2_id].weight
+        loss_weight = self.decoder.loss_fn[lang2_id].weight.clone()
         loss_weight[params.sos_index] = 0
         loss_weight[params.sep_index] = 0
+        loss_weight[params.pad_index] = 0
         loss_fn2 = nn.CrossEntropyLoss(weight=loss_weight, size_average=False)
 
         n_words2 = self.params.n_words[lang2_id]
@@ -390,10 +390,10 @@ class EvaluatorMT(object):
         with open(hyp_path_pad, 'w', encoding='utf-8') as f:
             f.write('\n'.join(txt_pad) + '\n')
 
-        hyp_name = 'hyp{0}.{1}-{2}.{3}.tone_ehance.txt'.format(scores['epoch'], lang1, lang2, data_type)
-        hyp_path = os.path.join(params.dump_path, hyp_name)
+        hyp_name_enh = 'hyp{0}.{1}-{2}.{3}.tone_ehance.txt'.format(scores['epoch'], lang1, lang2, data_type)
+        hyp_path_enh = os.path.join(params.dump_path, hyp_name_enh)
         # export sentences to hypothesis file / restore BPE segmentation
-        with open(hyp_path, 'w', encoding='utf-8') as f:
+        with open(hyp_path_enh, 'w', encoding='utf-8') as f:
             f.write('\n'.join(txt_tone_enh) + '\n')
         # restore_segmentation(hyp_path)
 
@@ -436,9 +436,10 @@ class EvaluatorMT(object):
         txt3 = []
 
         # for perplexity
-        loss_weight = self.decoder.loss_fn[lang3_id].weight
+        loss_weight = self.decoder.loss_fn[lang3_id].weight.clone()
         loss_weight[params.sos_index] = 0
         loss_weight[params.sep_index] = 0
+        loss_weight[params.pad_index] = 0
         loss_fn3 = nn.CrossEntropyLoss(weight=loss_weight, size_average=False)
         
         n_words3 = self.params.n_words[lang3_id]
@@ -587,9 +588,10 @@ class EvaluatorMT(object):
         txt_tone_enh = []
 
         # for perplexityloss_weight
-        loss_weight = self.decoder.loss_fn[lang2_id].weight
+        loss_weight = self.decoder.loss_fn[lang2_id].weight.clone()
         loss_weight[params.sos_index] = 0
         loss_weight[params.sep_index] = 0
+        loss_weight[params.pad_index] = 0
         loss_fn2 = nn.CrossEntropyLoss(weight=loss_weight, size_average=False)
         
         n_words2 = self.params.n_words[lang2_id]
@@ -611,7 +613,7 @@ class EvaluatorMT(object):
             encoded = self.encoder(sent1, len1, lang1_id)
             max_len = self.params.max_len[lang2_id]
             sent2_, len2_, _ = self.decoder.generate(encoded, lang2_id, max_len=max_len)
-            sent2_enh, len2_enh = self.double_para(sent2_, len2_, lang2_id, do_pad=self.params.do_pad, do_bos=self.params.do_bos, do_sep=self.params.do_sep)
+            # sent2_enh, len2_enh = self.double_para(sent2_, len2_, lang2_id, do_pad=self.params.do_pad, do_bos=self.params.do_bos, do_sep=self.params.do_sep)
 
             a, b = self.eval_rythm(sent2_, do_pad=self.params.do_pad, do_bos=self.params.do_bos, do_sep=self.params.do_sep)
             rytm_correct += a 
@@ -620,7 +622,7 @@ class EvaluatorMT(object):
 
             # convert to text
             txt.extend(convert_to_text(sent2_, len2_, self.dico[lang2], lang2_id, self.params, do_pad=self.params.do_pad, do_bos=self.params.do_bos, do_sep=self.params.do_sep))
-            txt_tone_enh.extend(convert_to_text(sent2_enh, len2_enh, self.dico[lang2], lang2_id, self.params, do_pad=self.params.do_pad, do_bos=self.params.do_bos, do_sep=self.params.do_sep))
+            # txt_tone_enh.extend(convert_to_text(sent2_enh, len2_enh, self.dico[lang2], lang2_id, self.params, do_pad=self.params.do_pad, do_bos=self.params.do_bos, do_sep=self.params.do_sep))
 
         # hypothesis / reference paths
         hyp_name = 'hyp{0}.{1}-{2}.{3}.txt'.format(scores['epoch'], lang1, lang2, data_type)
@@ -628,10 +630,10 @@ class EvaluatorMT(object):
         # export sentences to hypothesis file / restore BPE segmentation
         with open(hyp_path, 'w', encoding='utf-8') as f:
             f.write('\n'.join(txt) + '\n')
-        hyp_name = 'hyp{0}.{1}-{2}.{3}.tone_ehance.txt'.format(scores['epoch'], lang1, lang2, data_type)
-        hyp_path = os.path.join(params.dump_path, hyp_name)
+        hyp_name_enh = 'hyp{0}.{1}-{2}.{3}.tone_ehance.txt'.format(scores['epoch'], lang1, lang2, data_type)
+        hyp_path_enh = os.path.join(params.dump_path, hyp_name_enh)
         # export sentences to hypothesis file / restore BPE segmentation
-        with open(hyp_path, 'w', encoding='utf-8') as f:
+        with open(hyp_path_enh, 'w', encoding='utf-8') as f:
             f.write('\n'.join(txt_tone_enh) + '\n')
 
         # if bleu_eval:
diff --git a/src/model/seq2seq.py b/src/model/seq2seq.py
index d92b8d2..b53934d 100644
--- a/src/model/seq2seq.py
+++ b/src/model/seq2seq.py
@@ -56,6 +56,7 @@ class Encoder(nn.Module):
         self.share_enc = params.share_enc
         self.proj_mode = params.proj_mode
         self.pad_index = params.pad_index
+        self.unk_index = params.unk_index
         self.freeze_enc_emb = params.freeze_enc_emb
         assert not self.share_lang_emb or len(set(params.n_words)) == 1
         assert 0 <= self.share_enc <= self.n_enc_layers + int(self.proj_mode == 'proj')
@@ -423,7 +424,7 @@ def build_seq2seq_model(params, data, cuda=True):
     loss_fn = []
     for n_words in params.n_words:
         loss_weight = torch.FloatTensor(n_words).fill_(1)
-        loss_weight[params.pad_index] = 0
+        loss_weight[params.pad_index] = params.pad_weight #0
         loss_fn.append(nn.CrossEntropyLoss(loss_weight, size_average=True))
     decoder.loss_fn = nn.ModuleList(loss_fn)
 
diff --git a/src/model/transformer.py b/src/model/transformer.py
index a915ce6..ca1c833 100644
--- a/src/model/transformer.py
+++ b/src/model/transformer.py
@@ -98,7 +98,7 @@ class TransformerEncoder(nn.Module):
             x = self.embed_scale * embed_input.view(slen * bs, -1).mm(embed_tokens.weight).view(slen, bs, self.embed_dim)
             # x = self.embed_scale * embed_input.view(slen * bs, -1)[:,0:768].view(slen, bs, self.embed_dim)
         x = x.detach() if self.freeze_enc_emb else x
-        x += self.embed_positions(src_tokens, is_short=False) #not bool(lang_id)
+        x += self.embed_positions(src_tokens)
         x = F.dropout(x, p=self.dropout, training=self.training)      
         # logger.info(src_tokens.shape) #TODO
         # logger.info(x.shape)
@@ -229,7 +229,7 @@ class TransformerDecoder(nn.Module):
         proj_layer = self.proj[lang_id]
 
         # embed positions
-        positions = self.embed_positions(prev_output_tokens, incremental_state, is_short=False) #not bool(lang_id)
+        positions = self.embed_positions(prev_output_tokens, incremental_state)
 
         # embed tokens and positions
         if incremental_state is not None:
diff --git a/src/modules/sinusoidal_positional_embedding.py b/src/modules/sinusoidal_positional_embedding.py
index 6421a29..c795412 100644
--- a/src/modules/sinusoidal_positional_embedding.py
+++ b/src/modules/sinusoidal_positional_embedding.py
@@ -13,7 +13,7 @@ import torch
 import torch.nn as nn
 
 
-def make_positions(tensor, padding_idx, left_pad, is_short=False):
+def make_positions(tensor, padding_idx, left_pad):
     """Replace non-padding symbols with their position numbers.
     Position numbers begin at padding_idx+1.
     Padding symbols are ignored, but it is necessary to specify whether padding
@@ -27,8 +27,6 @@ def make_positions(tensor, padding_idx, left_pad, is_short=False):
         torch.arange(padding_idx + 1, max_pos, out=make_positions.range_buf)
     mask = tensor.ne(padding_idx)
     positions = make_positions.range_buf[:tensor.size(0)].unsqueeze(-1).expand(-1, tensor.size(1))
-    if is_short:
-        positions = positions*2
     if left_pad:
         positions = positions - mask.size(0) + mask.long().sum(dim=0).unsqueeze(0)
     return tensor.clone().masked_scatter_(mask, positions[mask])
@@ -72,7 +70,7 @@ class SinusoidalPositionalEmbedding(nn.Module):
             emb[padding_idx, :] = 0
         return emb
 
-    def forward(self, input, incremental_state=None, is_short=False):
+    def forward(self, input, incremental_state=None):
         """Input is expected to be of size [seqlen x bsz]."""
         # recompute/expand embeddings if needed
         seq_len, bsz = input.size()
@@ -90,5 +88,5 @@ class SinusoidalPositionalEmbedding(nn.Module):
             # positions is the same for every token when decoding a single step
             return weights[self.padding_idx + seq_len, :].expand(1, bsz, -1)
 
-        positions = make_positions(input.data, self.padding_idx, self.left_pad, is_short=is_short)
+        positions = make_positions(input.data, self.padding_idx, self.left_pad)
         return weights.index_select(0, positions.view(-1)).view(seq_len, bsz, -1)
diff --git a/src/trainer.py b/src/trainer.py
index e9b6857..d82f777 100644
--- a/src/trainer.py
+++ b/src/trainer.py
@@ -96,7 +96,7 @@ class TrainerMT(MultiprocessingEventLoop):
         # training variables
         self.best_metrics = {metric: -1e12 for metric in self.VALIDATION_METRICS}
         self.epoch = 0
-        self.n_total_iter = 0
+        self.n_total_iter = params.n_total_iter
         self.freeze_enc_emb = self.params.freeze_enc_emb
         self.freeze_dec_emb = self.params.freeze_dec_emb
 
diff --git a/test.py b/test.py
index 1b08bb4..6d2b9cf 100644
--- a/test.py
+++ b/test.py
@@ -6,28 +6,127 @@ import numpy as np
 import torch
 import collections
 
+PADDING_IDX=1
 
-def load_vocab_rev(vocab_file):
-    """Loads a vocabulary file into a dictionary."""
-    vocab = collections.OrderedDict()
-    index = 0
-    with io.open(vocab_file, "r", encoding="utf8") as reader:
-        while True:
-            token = reader.readline()
-            if not token:
-                break
-            token = token.strip()
-            vocab[index] = token
-            index += 1
-    return vocab
-
-def batch_sentences(sentences, lang_id):
+def get_data(input_sents, tokenizer, issanwen, dopmpad):
+    sent_str = []
+    positions = []
+    sentences = []
+    sentences_len = []
+    sent_str_abs = []
+    positions_abs = []
+    sentences_abs = []
+    sentences_abs_len = []
+    length_in_count = np.zeros(int(MAX_SENT_LEN/10)+1)
+    unk_words = {}
+    line_count=0
+    too_long_sent_count = 0
+    long_sent_count = 0
+    # for ind in range(len(input_sents)):
+    for ind in range(len(input_sents)):
+        sent=input_sents[ind]
+        if issanwen:
+            realmax_len = np.random.normal(loc=69.0, scale=10.0, size=None) #TODO: 99
+        else:
+            realmax_len=MAX_SENT_LEN
+        # realmax_len=MAX_SENT_LEN
+        
+        if realmax_len > MAX_SENT_LEN:
+            realmax_len = MAX_SENT_LEN 
+        realmax_len = int(realmax_len)
+
+        if len(sent) > realmax_len:
+            # print("Long sentence with len %i in line %i." % (len(sent),line_count))
+            sent=sent[0:realmax_len]
+            sent = list(zng(sent)) # ends with punc
+            if len(sent) == 0:
+                sent=''
+                too_long_sent_count+=1
+            else:
+                assert len(sent)==1
+                sent = sent[0]
+                long_sent_count+=1
+        token_s = tokenizer.tokenize(sent)
+        # if len(token_s) == 0:
+        #     print("Empty sentence in line %i." % line_count)
+        if len(token_s) > 21: #TODO: 31
+            # index sentence words
+            indexed = tokenizer.convert_tokens_to_ids(token_s)
+            unk_idxs = [i for i, e in enumerate(indexed) if e == 100]
+            for unk_idx in unk_idxs:
+                w = sent[unk_idx] 
+                unk_words[w] = unk_words.get(w, 0) + 1
+            if dopmpad:
+                ind_len = len(indexed)
+                indexed = np.array(indexed)
+                sliced = list(range(2,ind_len+1,2))+list(range(2,ind_len+1,2))
+                # logger.info(ind)
+                # logger.info(indexed.shape)
+                indexed = np.insert(indexed, sliced, [PADDING_IDX]*ind_len)
+                # logger.info(indexed.shape)
+                # logger.info(indexed)
+
+            # add sentence
+            sent_str.append(sent)
+            positions.append([len(sentences), len(sentences) + len(indexed)])
+            sentences_len.append(len(indexed))
+            sentences.extend(indexed)
+            sentences.append(-1)
+
+            if issanwen:
+                summary = shorten_sents(sent, min_len=31, max_len=45)
+                token_s_abs = tokenizer.tokenize(summary)
+                indexed_abs = tokenizer.convert_tokens_to_ids(token_s_abs)
+                sent_str_abs.append(summary)
+                positions_abs.append([len(sentences_abs), len(sentences_abs) + len(indexed_abs)])
+                sentences_abs_len.append(len(indexed_abs))
+                sentences_abs.extend(indexed_abs)
+                sentences_abs.append(-1)
+
+            line_count+=1
+            if len(token_s) > MAX_SENT_LEN:
+                length_in_count[-1] += 1
+            else:
+                length_in_count[int(len(token_s)/10)] += 1
+        # else:
+        #     print("Short sentence in line %i. <=10" % line_count)
+
+
+    # tensorize data
+    positions = torch.LongTensor(positions)
+    sentences = torch.LongTensor(sentences)
+    positions_abs = torch.LongTensor(positions_abs)
+    sentences_abs = torch.LongTensor(sentences_abs)
+    data = {
+        'dico': tokenizer,
+        'positions': positions,
+        'sentences': sentences,
+        'positions_abs': positions_abs,
+        'sentences_abs': sentences_abs,
+        'unk_words': unk_words,
+    }
+    logger.info('long sentence count:')
+    logger.info(long_sent_count)
+    logger.info('long sentence that can not convert count:')
+    logger.info(too_long_sent_count)
+    length_in_count = length_in_count/np.sum(length_in_count)
+    logger.info('sentence length bin count:')
+    logger.info(length_in_count)
+    logger.info('sentence length mean and std:')
+    logger.info(np.mean(sentences_len))
+    logger.info(np.std(sentences_len))
+    if issanwen:
+        logger.info('abstract sentence length mean and std:')
+        logger.info(np.mean(sentences_abs_len))
+        logger.info(np.std(sentences_abs_len))
+    return data, sent_str, sent_str_abs
+
+def batch_sentences_pm(sentences):
     """
     Take as input a list of n sentences (torch.LongTensor vectors) and return
     a tensor of size (s_len, n) where s_len is the length of the longest
     sentence, and a vector lengths containing the length of each sentence.
     """
-    assert type(lang_id) is int
     lengths = torch.LongTensor([len(s) + 2 for s in sentences])
     sent = torch.LongTensor(lengths.max(), lengths.size(0)).fill_(0)
     sent[0] = 4
@@ -38,50 +137,50 @@ def batch_sentences(sentences, lang_id):
 
 
 # data = torch.load('./data/data_pad/jueju7_out.vl.pth')
-tone_file = './data/vocab_tone_pure.txt'
-data = torch.load('./data/data_acc/poem_jueju7_para.pm.pth')
-data['sentences'] = data['sentences'].numpy()
+dopmpad = False
+epoch = 0
+lang1 = 'sw'
+lang1 = 'pm'
+data_type = 'test1'
+txt_path = '...'
+tokenizer = ...
+
+valid_input = []
+with io.open(txt_path, "r", encoding='utf8') as f:
+    for line in f:
+        s = line.rstrip()
+        valid_input.append(txt_path)
+
+data, sent, sent_abs = get_data(valid_input, tokenizer, False, dopmpad)
+
 data['positions'] = data['positions'].numpy()
-dictionary = data['dico']
 n_sentences = len(data['positions'])
 indices = np.arange(n_sentences)
 batches = np.array_split(indices, math.ceil(len(indices) * 1. / 32))
 
-dictionary.ids_to_tones = load_vocab_rev(tone_file)
-print(dictionary.ids_to_tones)
-
+txt_tone_enh = []
 for sentence_ids in batches:
-  pos1 = data['positions'][sentence_ids]
-  sents = [data['sentences'][a:b] for a, b in pos1]
-  for sent in sents:
-    print (dictionary.convert_ids_to_tones(sent))
-
-
-# ['1', '2', '1', '1', '1', '2', '2', '0', '1', '1', '2', '2', '1', '1', '2', '0',
-#  '1', '1', '1', '2', '2', '1', '1', '0', '1', '2', '2', '1', '1', '2', '2', '0']
-# ['1', '1', '3', '2', '1', '1', '2', '0', '2', '2', '2', '1', '1', '2', '2', '0',
-#  '1', '2', '1', '1', '2', '2', '1', '0', '1', '1', '2', '2', '1', '1', '2', '0']
-# ['1', '1', '2', '2', '1', '1', '1', '0', '2', '2', '2', '1', '1', '1', '2', '0',
-#  '2', '2', '1', '1', '3', '2', '1', '0', '2', '1', '2', '2', '1', '1', '2', '0']
-# ['2', '1', '2', '2', '1', '0', '2', '0', '2', '2', '2', '1', '1', '1', '2', '0',
-#  '2', '1', '1', '1', '2', '2', '1', '0', '1', '1', '2', '2', '1', '1', '2', '0']
-# ['1', '1', '2', '2', '1', '1', '2', '0', '2', '2', '1', '1', '1', '2', '2', '0',
-#  '1', '3', '1', '1', '2', '3', '1', '0', '1', '1', '2', '2', '1', '1', '2', '0']
-
-['1', '2', '1', '1', '1', '2', '2', '0', '1', '1', '2', '2', '1', '1', '2', '0',
- '1', '1', '1', '2', '2', '1', '3', '0', '1', '2', '2', '1', '1', '2', '2', '0']
-['3', '1', '3', '2', '1', '1', '2', '0', '2', '2', '2', '1', '1', '2', '2', '0',
- '1', '2', '1', '1', '2', '2', '1', '0', '1', '1', '2', '2', '3', '1', '2', '0']
-['1', '1', '2', '2', '1', '3', '1', '0', '2', '2', '2', '1', '1', '1', '2', '0',
- '2', '2', '1', '1', '3', '2', '1', '0', '2', '1', '2', '2', '1', '1', '2', '0']
-['2', '1', '2', '2', '1', '0', '2', '0', '2', '2', '2', '1', '1', '1', '2', '0',
- '2', '1', '1', '1', '2', '2', '1', '0', '3', '1', '2', '2', '1', '1', '2', '0']
-['1', '1', '2', '2', '1', '1', '2', '0', '2', '2', '3', '1', '1', '2', '2', '0',
- '1', '3', '1', '1', '2', '3', '1', '0', '1', '1', '2', '2', '1', '1', '2', '0']
-
-# 近寒食雨草萋萋，著麦苗风柳映堤。等是有家归未得，杜鹃休向耳边啼。
-# 绝域从军计惘然，东南幽恨满词笺。一箫一剑平生意，负尽狂名十五年。
-# 莫道秋江离别难，舟船明日是长安。吴姬缓舞留君醉，随意青枫白露寒。
-# 鹦鹉洲头浪飐沙，青楼春望日将斜。衔泥燕子争归舍，独自狂夫不忆家。
-# 一自萧关起战尘，河湟隔断异乡春。汉儿尽作胡儿语，却向城头骂汉人。
-# 溪水将桥不复回，小舟犹倚短篙开。交情得似山溪渡，不管风波去又来。
\ No newline at end of file
+  pos = data['positions'][sentence_ids]
+  sents = [data['sentences'][a:b] for a, b in pos]
+  sent2_, len2_ = batch_sentences_pm(sents)
+  lang2_id = 0
+  sent2_enh, len2_enh = double_para(sent2_, len2_, lang2_id, do_pad=False, do_bos=self.params.do_bos, do_sep=self.params.do_sep)
+  txt_tone_enh.extend(convert_to_text(sent2_enh, len2_enh, self.dico[lang2], lang2_id, self.params, do_pad=False, do_bos=self.params.do_bos, do_sep=self.params.do_sep))
+
+hyp_name_enh = 'hyp{0}.{1}-{2}.{3}.tone_ehance.txt'.format(epoch, lang1, lang2, data_type)
+hyp_path_enh = os.path.join(params.dump_path, hyp_name_enh)
+# export sentences to hypothesis file / restore BPE segmentation
+with open(hyp_path_enh, 'w', encoding='utf-8') as f:
+    f.write('\n'.join(txt_tone_enh) + '\n')
+
+
+# import jieba
+# txt_path = './data/data_pad/jueju7_out.vl.txt'
+# valid_input = []
+# with io.open(txt_path, "r", encoding='utf8') as f:
+#     for line in f:
+#         s = line.rstrip()
+#         seg_list = jieba.cut(s)
+#         valid_input.append(seg_list)
+#         print("Default Mode: " + "/ ".join(seg_list))  # 精确模式
+
